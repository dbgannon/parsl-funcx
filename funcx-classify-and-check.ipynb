{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Checking the Bert Model \n",
    "In this notebook we load the SCIML Arxiv data set and test the Bert Model saved in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "The follow set of functions load the data.  the files are in the folder classifiers/sci_doc.  There are two files there.   sciml_data_ariv.p and convig_all4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/'\n",
    "def load_docs(path, name):\n",
    "    filename = path+name+\".p\"\n",
    "    fileobj = open(filename, \"rb\")\n",
    "    z = fileobj.read()\n",
    "    lst = pickle.loads(z)\n",
    "    titles = []\n",
    "    sitenames = []\n",
    "    abstracts = []\n",
    "    for i in range(0, len(lst)):\n",
    "        titles.extend([lst[i][0]])\n",
    "        sitenames.extend([lst[i][1]])\n",
    "        abstracts.extend([lst[i][2]])\n",
    "        \n",
    "    #print(\"done loading \"+filename)\n",
    "    return abstracts, sitenames, titles\n",
    "\n",
    "\n",
    "def read_config(subj, basepath):\n",
    "    docpath =basepath+ \"/config_\"+subj+\".json\"\n",
    "    with open(docpath, 'rb') as f:\n",
    "        doc = f.read() \n",
    "    z =json.loads(doc)\n",
    "    subject = z['subject']\n",
    "    loadset = z['loadset']\n",
    "    subtopics = []\n",
    "    for w in z['supertopics']:\n",
    "        subtopics.extend([(w[0], w[1])])\n",
    "    return subject, loadset, subtopics\n",
    "\n",
    "def make_dict(subtopics):\n",
    "    dic = {}\n",
    "    for main in subtopics:\n",
    "        sl = main[1]\n",
    "        for x in sl:\n",
    "            dic[x] = main[0]\n",
    "    return dic\n",
    "\n",
    "def split_titles(disp_title):\n",
    "    subject,loadset, subtopics = read_config(\"all4\",root+\"classifier/sci_doc\")\n",
    "    dic = make_dict(subtopics)\n",
    "    lis = []\n",
    "    for ti in disp_title:\n",
    "        l = ti.find('[')\n",
    "        if(l >= 0):\n",
    "            #lis.append(ti[:l])\n",
    "            e = ti[l+1:]\n",
    "            l2 = e.find(']')\n",
    "            e = e[:l2]\n",
    "            try:\n",
    "                if dic[e]== 'compsci':\n",
    "                    lis.append([ti[:l], 0, e])\n",
    "                if dic[e]== 'math':\n",
    "                    lis.append([ti[:l], 1, e])\n",
    "                if dic[e]== 'Physics':\n",
    "                    lis.append([ti[:l], 2, e])\n",
    "                if dic[e]== 'bio':\n",
    "                    lis.append([ti[:l], 3, e])\n",
    "                if dic[e]== 'finance':\n",
    "                    lis.append([ti[:l], 4, e])\n",
    "            except:\n",
    "                e = 1\n",
    "            \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Getting the data\n",
    "there are three items that are loaded\n",
    "\n",
    "abstracts - the text of the abstracts\n",
    "\n",
    "sitenames - the names of the arxiv categories\n",
    "\n",
    "disp_titles - titles of the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './'\n",
    "abstracts, sitenames, disp_title = load_docs(root+\"classifier/sci_doc/\", \"sciml_data_arxiv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep convolutional neural networks (CNN) have recently been shown in many computer vision and pattern recog- nition applications to outperform by a significant margin state- of-the-art solutions that use traditional hand-crafted features. However, this impressive performance is yet to be fully exploited in robotics. In this paper, we focus one specific problem that can benefit from the recent development of the CNN technology, i.e., we focus on using a pre-trained CNN model as a method of generating an image representation appropriate for visual loop closure detection in SLAM (simultaneous localization and mapping). We perform a comprehensive evaluation of the outputs at the intermediate layers of a CNN as image descriptors, in comparison with state-of-the-art image descriptors, in terms of their ability to match images for detecting loop closures. The main conclusions of our study include: (a) CNN-based image representations perform comparably to state-of-the-art hand- crafted competitors in environments without significant lighting change, (b) they outperform state-of-the-art competitors when lighting changes significantly, and (c) they are also significantly faster to extract than the state-of-the-art hand-crafted features even on a conventional CPU and are two orders of magnitude faster on an entry-level GPU.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cs.RO'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sitenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convolutional Neural Network-Based Image Representation for Visual Loop   Closure Detection [cs.RO]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disp_title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_text will create list of pairs (abstract, true_index) where true_index is based on the topic of the astract.  the key is\n",
    "\n",
    "0 = computer science\n",
    "\n",
    "1 = math\n",
    "\n",
    "2 = physics \n",
    "\n",
    "3 = bio\n",
    "\n",
    "4 = finance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(disp_title, abstracts):\n",
    "    subject,loadset, subtopics = read_config(\"all4\",root+\"/classifier/sci_doc\")\n",
    "    dic = make_dict(subtopics)\n",
    "    lis = []\n",
    "    ind = 0\n",
    "    for ind in range(len(disp_title)): #disp_title,titles:\n",
    "        ti = disp_title[ind]\n",
    "        te = abstracts[ind]\n",
    "        l = ti.find('[')\n",
    "        if(l >= 0):\n",
    "            #lis.append(ti[:l])\n",
    "            e = ti[l+1:]\n",
    "            l2 = e.find(']')\n",
    "            e = e[:l2]\n",
    "            try:\n",
    "                if dic[e]== 'compsci':\n",
    "                    lis.append([te, 0])\n",
    "                if dic[e]== 'math':\n",
    "                    lis.append([te, 1])\n",
    "                if dic[e]== 'Physics':\n",
    "                    lis.append([te, 2])\n",
    "                if dic[e]== 'bio':\n",
    "                    lis.append([te, 3])\n",
    "                if dic[e]== 'finance':\n",
    "                    lis.append([te, 4])\n",
    "            except:\n",
    "                e = 1\n",
    "            \n",
    "    return lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training and test sets\n",
    "we use list elements 0-4499 as the training set and 4500-7109 as the test set.  we are not training here so we really only need the test set and we convert that into a pandas data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_titles = split_titles(disp_title[0:4500]) #contains titles only\n",
    "eval_titles  = split_titles(disp_title[4500:])\n",
    "#train_text = split_text(disp_title[0:4500], abstracts[0:4500]) #this is the training set\n",
    "eval_text = split_text(disp_title[4500:],abstracts[4500:])  #contains text + class\n",
    "text_eval_df = pd.DataFrame(eval_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2609"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(disp_title[4500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcx.sdk.client import FuncXClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the connection to the container running in a small desktop kub cluster.  \n",
    "we already have the end point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "myep = 'd532a334-beaf-4aa1-a8e5-9a9594409152'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fxc = FuncXClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def funcx_imp(stringlist):\n",
    "    import sys\n",
    "    sys.path.append('/')\n",
    "    from classifier import classifys\n",
    "    x = classifys(stringlist)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We show that effective theories of matter that classically violate the null energy condition cannot be minimally coupled to Einstein gravity without being inconsistent with both string theory and black hole thermodynamics. We argue however that they could still be either non-minimally coupled or coupled to higher-curvature theories of gravity.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = text_eval_df[0][2604]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "go2_function = fxc.register_function(funcx_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "res = fxc.run([s], endpoint_id=myep, function_id=go2_function)\n",
    "print(fxc.get_task(res)['pending'])\n",
    "while fxc.get_task(res)['pending']:\n",
    "    time.sleep(3)\n",
    "    print(fxc.get_task(res)['pending'])\n",
    "print (fxc.get_result(res))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_eval_df[1] is the column of the dataframe that give the correct answer.  in this case it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_eval_df[1][2604]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run big tests\n",
    "we will do the test in several batches because we aree using the classifys function which menans we are sending the entire text of each abstract.  we are starting with 1200 to 14500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inlist = [text_eval_df[0][i] for i in range(1200,1450)]\n",
    "chklist = [text_eval_df[1][i] for i in range(1200,1450)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "[1 2 2 2 4 2 0 4 1 2 2 2 1 1 2 2 1 2 2 2 1 2 2 1 0 2 0 3 2 1 4 2 1 0 3 1 0\n",
      " 2 2 1 2 3 0 1 2 1 1 2 1 4 0 1 2 2 2 0 0 3 0 2 0 2 2 0 2 1 0 3 1 2 0 2 2 2\n",
      " 3 0 3 0 3 2 2 2 0 1 4 2 4 2 3 0 2 1 2 2 2 0 3 0 4 1 1 2 1 2 1 3 0 2 2 2 1\n",
      " 2 2 2 0 2 2 3 2 4 0 2 3 0 1 3 0 2 0 2 2 1 2 0 1 2 1 2 2 2 0 0 1 2 2 2 0 0\n",
      " 3 2 0 1 2 2 4 0 3 4 0 2 1 2 2 3 2 2 3 0 2 2 0 4 2 0 0 2 2 1 2 2 2 1 2 1 2\n",
      " 0 4 4 2 2 1 2 0 1 3 0 2 2 1 0 2 1 2 0 2 0 3 2 4 2 1 0 2 0 0 2 2 2 2 2 0 1\n",
      " 0 1 2 1 2 3 0 4 2 2 1 0 2 0 0 3 2 1 0 2 2 4 0 3 3 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "res = fxc.run(inlist, endpoint_id=myep, function_id=go2_function)\n",
    "print(fxc.get_task(res)['pending'])\n",
    "while fxc.get_task(res)['pending']:\n",
    "    time.sleep(3)\n",
    "    print(fxc.get_task(res)['pending'])\n",
    "print (fxc.get_result(res))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82\n"
     ]
    }
   ],
   "source": [
    "out = fxc.get_result(res)\n",
    "correct = 0\n",
    "for i in range(len(chklist)):\n",
    "    if out[i] == chklist[i]:\n",
    "        correct+=1\n",
    "print(correct/len(chklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "[2 2 2 ... 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "inlist = [text_eval_df[0][i] for i in range(1450,2604)]\n",
    "chklist = [text_eval_df[1][i] for i in range(1450,2604)]\n",
    "res = fxc.run(inlist, endpoint_id=myep, function_id=go2_function)\n",
    "print(fxc.get_task(res)['pending'])\n",
    "while fxc.get_task(res)['pending']:\n",
    "    time.sleep(10)\n",
    "    print(fxc.get_task(res)['pending'])\n",
    "print (fxc.get_result(res))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7902946273830156\n"
     ]
    }
   ],
   "source": [
    "out = fxc.get_result(res)\n",
    "correct = 0\n",
    "for i in range(len(chklist)):\n",
    "    if out[i] == chklist[i]:\n",
    "        correct+=1\n",
    "print(correct/len(chklist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "[0 1 0 ... 2 3 0]\n"
     ]
    }
   ],
   "source": [
    "inlist = [text_eval_df[0][i] for i in range(0,1200)]\n",
    "chklist = [text_eval_df[1][i] for i in range(0,1200)]\n",
    "res = fxc.run(inlist, endpoint_id=myep, function_id=go2_function)\n",
    "print(fxc.get_task(res)['pending'])\n",
    "while fxc.get_task(res)['pending']:\n",
    "    time.sleep(30)\n",
    "    print(fxc.get_task(res)['pending'])\n",
    "print (fxc.get_result(res))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216666666666667\n"
     ]
    }
   ],
   "source": [
    "out = fxc.get_result(res)\n",
    "correct = 0\n",
    "for i in range(len(chklist)):\n",
    "    if out[i] == chklist[i]:\n",
    "        correct+=1\n",
    "print(correct/len(chklist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7610829493087559"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(.82*250 - .79*(1450-2604) + .721*1200)/(250+(2604-1450)+1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "76% correct on a 1 out 5 choice test is not bad.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
